{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer)\n",
    "\n",
    "ここ数年の最先端の自然言語処理(Natural Language Processing, NLP)ではTransformerという技術が広く応用されている．\n",
    "Transformerは[Attention Is All You Need](https://arxiv.org/abs/1706.03762)で提案された手法である．\n",
    "[The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer)では，注釈付きでこの論文を解説している．\n",
    "自分のメモ用に，この内容を要約してここに記しておく．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some convenience helper functions used throughout the notebook\n",
    "\n",
    "\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "NLPにおいて生じる逐次計算(sequential computation)の演算回数を減らすことが重要である．\n",
    "かつて提案されたConvS2SやByteNetでは入力する単語数が増加するにつれて，計算量が大幅に増加するという課題があった．\n",
    "一方で，Transformerでは，Multi-Head Attentionという手法により，計算量を減らすことができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Model Architecture\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "NLPにおける[trunsduction model](https://en.wikipedia.org/wiki/Transduction_(machine_learning))(非ラベル化されたデータを教師無し学習によってラベル付けする手法)では，エンコーダ-デコーダ構造(encoder-decoder model structure)がしばしば採用されている．\n",
    "この手法では，単語列のようなシンボル列$(x_1 ,...,x_n)$を一旦，連続値(z_1,...,z_n)に変換(エンコード)したのち，最終的に(y_1,...,y_n)を出力(デコード)するというものである．\n",
    "このエンコーダ-デコーダ構造がトップレベルで見た時の構造になるので，ここから見ていく．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        # 入力xをエンコードしてzを生成 -> デコードして出力yを生成\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe68960602b458c964a5bab0fa4808408af13417cd7c4eab55f09dce085a7b9e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('kaggle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
