{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer)\n",
    "\n",
    "ここ数年の最先端の自然言語処理(Natural Language Processing, NLP)ではTransformerという技術が広く応用されている．\n",
    "Transformerは[Attention Is All You Need](https://arxiv.org/abs/1706.03762)で提案された手法である．\n",
    "[The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer)では，注釈付きでこの論文を解説している．\n",
    "自分のメモ用に，この内容を要約してここに記しておく．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/yutahirai/Desktop/Github/ReiRev/machine-learning-study/transformers/TheAnnotatedTransformer.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yutahirai/Desktop/Github/ReiRev/machine-learning-study/transformers/TheAnnotatedTransformer.ipynb#ch0000002?line=13'>14</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m RUN_EXAMPLES:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yutahirai/Desktop/Github/ReiRev/machine-learning-study/transformers/TheAnnotatedTransformer.ipynb#ch0000002?line=14'>15</a>\u001b[0m         fn(\u001b[39m*\u001b[39margs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yutahirai/Desktop/Github/ReiRev/machine-learning-study/transformers/TheAnnotatedTransformer.ipynb#ch0000002?line=17'>18</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDummyOptimizer\u001b[39;00m(torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mOptimizer):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yutahirai/Desktop/Github/ReiRev/machine-learning-study/transformers/TheAnnotatedTransformer.ipynb#ch0000002?line=18'>19</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yutahirai/Desktop/Github/ReiRev/machine-learning-study/transformers/TheAnnotatedTransformer.ipynb#ch0000002?line=19'>20</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m}]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Some convenience helper functions used throughout the notebook\n",
    "\n",
    "\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "NLPにおいて生じる逐次計算(sequential computation)の演算回数を減らすことが重要である．\n",
    "かつて提案されたConvS2SやByteNetでは入力する単語数が増加するにつれて，計算量が大幅に増加するという課題があった．\n",
    "一方で，Transformerでは，Multi-Head Attentionという手法により，計算量を減らすことができる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe68960602b458c964a5bab0fa4808408af13417cd7c4eab55f09dce085a7b9e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('kaggle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
