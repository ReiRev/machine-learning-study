{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer():\n",
    "    def __init__(self, size, mean, std):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size, scale=(0.5, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HymenopteraDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transformer, phase):\n",
    "        self.file_list = glob.glob(os.path.join('./data/hymenoptera_data', phase, '**/*.jpg'))\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_list[index]\n",
    "        img_transformed = self.transformer(Image.open(img_path))\n",
    "        label = img_path.split('/')[-2]\n",
    "        label = 0 if label=='ants' else 1\n",
    "        return img_transformed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 224\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HymenopteraDataset(Transformer(size, mean, std), 'train')\n",
    "val_dataset = HymenopteraDataset(Transformer(size, mean, std), 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[6] = nn.Linear(in_features=4096, out_features=2, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0059,  0.0110, -0.0134,  ..., -0.0090, -0.0046,  0.0031],\n",
       "         [ 0.0114,  0.0006, -0.0043,  ..., -0.0094, -0.0103, -0.0088]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0084,  0.0092], requires_grad=True)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_name_to_update = ['classifier.6.weight', 'classifier.6.bias']\n",
    "param_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if name in param_name_to_update:\n",
    "        param.requires_grad = True\n",
    "        param_to_update.append(param)\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "param_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(param_to_update, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_dict = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6769014000892639\n",
      "0.5217145085334778\n",
      "0.40347787737846375\n",
      "0.29061099886894226\n",
      "0.1954514980316162\n",
      "0.21213524043560028\n",
      "0.29314523935317993\n",
      "0.2170630544424057\n",
      "0.14757533371448517\n",
      "0.15566052496433258\n",
      "0.09762852638959885\n",
      "0.1305747777223587\n",
      "0.18618687987327576\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 2):\n",
    "    for phase in ['train', 'val']:\n",
    "        model.train if phase=='train' else model.eval()\n",
    "        for inputs, labels in dataloader_dict[phase]:\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                if phase=='train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4279,  0.6499],\n",
       "        [ 0.3155, -0.4879],\n",
       "        [ 0.2264,  0.9292],\n",
       "        [-0.2829, -0.1526],\n",
       "        [ 0.0232,  0.6556],\n",
       "        [-0.6916,  0.5403],\n",
       "        [ 0.4781,  0.3159],\n",
       "        [ 1.6673,  0.3508],\n",
       "        [ 0.4930,  0.7440],\n",
       "        [-0.1169,  1.0106],\n",
       "        [ 0.4786,  0.2946],\n",
       "        [ 0.5993,  0.2022],\n",
       "        [ 0.8030,  0.5003],\n",
       "        [ 0.2679,  0.1262],\n",
       "        [-0.7878,  0.5535],\n",
       "        [ 0.0423,  0.7180],\n",
       "        [ 0.2794,  0.7567],\n",
       "        [ 0.0873,  0.6222],\n",
       "        [ 1.2246,  1.0483],\n",
       "        [ 0.4617,  1.4134],\n",
       "        [ 0.3000, -0.0237],\n",
       "        [ 0.1900,  0.6673],\n",
       "        [-0.7841,  1.4829],\n",
       "        [ 0.8830,  0.0101],\n",
       "        [-0.2396,  0.7908],\n",
       "        [-0.1861,  1.0922],\n",
       "        [-0.0040,  1.2910],\n",
       "        [-0.1528,  1.9742],\n",
       "        [ 0.9849,  0.6708],\n",
       "        [ 0.4597, -0.1098],\n",
       "        [-0.2933,  1.0022],\n",
       "        [-1.1290,  0.9574]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80c1ff4db5646da1a3ab76adc78ddd186ba08af5fe476060e77231de69b1ab77"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
